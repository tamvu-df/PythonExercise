{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tracemalloc\n",
    "def log_execution (func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        tracemalloc.start()\n",
    "        start  = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        print(f\"Function: {func.__name__}, Time taken: {end - start} seconds\")\n",
    "        print(f\"Function: {func.__name__}, Memory used: {current} bytes, Peak Memory used: {peak} bytes\")\n",
    "        tracemalloc.stop()\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_execution\n",
    "def find_character (input_string, character):\n",
    "    for i in range(len(input_string)):\n",
    "        if input_string[i] == character:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: find_character, Time taken: 0.0 seconds\n",
      "Function: find_character, Memory used: 72 bytes, Peak Memory used: 152 bytes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_character(\"snfdskfnjsddsjqwieuqweqw\", 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat(num_times):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            result = []\n",
    "            for _ in range(num_times):\n",
    "                result.append(func(*args, **kwargs))\n",
    "\n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@repeat(num_times=5)\n",
    "def find_character (input_string, character):\n",
    "    for i in range(len(input_string)):\n",
    "        if input_string[i] == character:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False, False, False]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_character(\"ddfdfsdfsd\", \"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not using generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_file_no_yield():\n",
    "    with open(\"large_log_file.txt\", 'r') as file:\n",
    "        return file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_execution\n",
    "def search_error():\n",
    "    lines = read_file_no_yield()\n",
    "    for line in lines:\n",
    "        if \"ERROR\" in line:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: search_error, Time taken: 16.613926649093628 seconds\n",
      "Function: search_error, Memory used: 296718 bytes, Peak Memory used: 954409618 bytes\n"
     ]
    }
   ],
   "source": [
    "search_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_file_with_yield():\n",
    "    with open(\"large_log_file.txt\") as file:\n",
    "        for line in file:\n",
    "            yield line.strip()\n",
    "        return file\n",
    "@log_execution\n",
    "def search_error():\n",
    "    for line in read_file_with_yield():\n",
    "        if \"ERROR\" in line:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: search_error, Time taken: 14.873018026351929 seconds\n",
      "Function: search_error, Memory used: 149253 bytes, Peak Memory used: 183623 bytes\n"
     ]
    }
   ],
   "source": [
    "search_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MultiThreading: \tMultiple threads within the same process, sharing memory space.\n",
    "    - Best for: I/O-bound tasks, such as reading/writing files, network requests, and user interfaces where waiting for external resources is common.\n",
    "- Multiprocessing: Multiple processes with separate memory spaces, each running its own Python interpreter.\n",
    "    - Best for: CPU-bound tasks, such as mathematical calculations, image processing, and database operations where parallelism is beneficial.\n",
    "- Asynchronous Programming: Single-threaded event loop managing tasks via cooperative multitasking (tasks yield control).\n",
    "    - Best for: I/O-bound tasks like handling large numbers of network connections, APIs, real-time systems, and web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import aiofiles\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(directory = \"files\"):\n",
    "    if os.path.exists(directory):\n",
    "    # If it exists, delete all files in the directory\n",
    "        for filename in os.listdir(directory):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)  # Remove the file\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)  # Remove the sub-directory\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "    else:\n",
    "        # If the directory does not exist, create it\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate 1000 text files with random numbers of lines\n",
    "@log_execution\n",
    "def generate_files(directory = \"files\"):\n",
    "    for i in range(1000):\n",
    "        file_name = f\"{directory}/file_{i+1}.txt\"\n",
    "        num_lines = random.randint(1, 500)  # Random number of lines between 1 and 500\n",
    "        with open(file_name, 'w') as f:\n",
    "            for _ in range(num_lines):\n",
    "                f.write(\"This is a line of text.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: generate_files, Time taken: 1.8480300903320312 seconds\n",
      "Function: generate_files, Memory used: 297267 bytes, Peak Memory used: 339181 bytes\n"
     ]
    }
   ],
   "source": [
    "generate_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not using asynchronisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create result.txt to store the number of lines for each file\n",
    "@log_execution\n",
    "def create_result_txt(path = \"result.txt\", directory = \"files\"):\n",
    "    with open(path, 'w') as result_file:\n",
    "        for i in range(1000):\n",
    "            file_name = f\"{directory}/file_{i+1}.txt\"\n",
    "            with open(file_name, 'r') as f:\n",
    "                lines =  f.readlines() \n",
    "                num_lines = len(lines)\n",
    "            result_file.write(f\"{file_name}: {num_lines} lines\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: create_result_txt, Time taken: 5.3719236850738525 seconds\n",
      "Function: create_result_txt, Memory used: 300384 bytes, Peak Memory used: 414971 bytes\n"
     ]
    }
   ],
   "source": [
    "create_result_txt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using asynchronisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_execution\n",
    "async def create_result_txt_with_async(path=\"result.txt\", directory=\"files\"):\n",
    "    async with aiofiles.open(path, 'w') as result_file:\n",
    "        for i in range(1000):\n",
    "            file_name = f\"{directory}/file_{i+1}.txt\"\n",
    "            async with aiofiles.open(file_name, 'r') as f:\n",
    "                lines = await f.readlines() \n",
    "                num_lines = len(lines) \n",
    "            await result_file.write(f\"{file_name}: {num_lines} lines\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: create_result_txt_with_async, Time taken: 0.0 seconds\n",
      "Function: create_result_txt_with_async, Memory used: 296 bytes, Peak Memory used: 296 bytes\n"
     ]
    }
   ],
   "source": [
    "await create_result_txt_with_async()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using multi-threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Create a lock for writing to the result file\n",
    "lock = threading.Lock()\n",
    "def read_length(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        lines =  f.readlines() \n",
    "        num_lines = len(lines)\n",
    "    return num_lines\n",
    "# Function to write the result for a file to the result.txt\n",
    "def write_result(file_name, num_lines, result_file):\n",
    "    with lock:  # Ensure only one thread writes at a time\n",
    "        result_file.write(f\"{file_name}: {num_lines} lines\\n\")\n",
    "\n",
    "# Function to process a range of files\n",
    "def process_files(start, end, directory, result_file):\n",
    "    for i in range(start, end):\n",
    "        file_name = f\"{directory}/file_{i+1}.txt\"\n",
    "        num_lines = read_length(file_name)\n",
    "        write_result(file_name, num_lines, result_file)\n",
    "\n",
    "@log_execution\n",
    "def create_result_txt(path=\"result.txt\", directory=\"files\", num_threads=5):\n",
    "    total_files = 1000\n",
    "    files_per_thread = total_files // num_threads\n",
    "    threads = []\n",
    "    # Open the result file in append mode\n",
    "    with open(path, 'w') as result_file:\n",
    "        # Create threads\n",
    "        for i in range(num_threads):\n",
    "            start = i * files_per_thread\n",
    "            # Ensure the last thread handles any remaining files\n",
    "            end = total_files if i == num_threads - 1 else (i + 1) * files_per_thread\n",
    "\n",
    "            # Create a thread to process a subset of files\n",
    "            thread = threading.Thread(target=process_files, args=(start, end, directory, result_file))\n",
    "            threads.append(thread)\n",
    "\n",
    "        # Start all threads\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "\n",
    "        # Wait for all threads to finish\n",
    "        for thread in threads:\n",
    "            thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: create_result_txt, Time taken: 0.36475372314453125 seconds\n",
      "Function: create_result_txt, Memory used: 730175 bytes, Peak Memory used: 958948 bytes\n"
     ]
    }
   ],
   "source": [
    "create_result_txt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import threading\n",
    "\n",
    "# Create a lock for writing to the result file\n",
    "lock = threading.Lock()\n",
    "\n",
    "def read_length(file_name):\n",
    "    try:\n",
    "        with open(file_name, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            num_lines = len(lines)\n",
    "        return num_lines\n",
    "    except FileNotFoundError:\n",
    "        return 0  # Handle case where file does not exist\n",
    "\n",
    "# Function to write the result for a file to the result.txt\n",
    "def write_result(file_name, num_lines, result_file):\n",
    "    with lock:  # Ensure only one thread writes at a time\n",
    "        result_file.write(f\"{file_name}: {num_lines} lines\\n\")\n",
    "\n",
    "# Function to process a single file\n",
    "def process_file(i, directory):\n",
    "    file_name = f\"{directory}/file_{i+1}.txt\"\n",
    "    num_lines = read_length(file_name)\n",
    "    return file_name, num_lines\n",
    "\n",
    "# Main function using ThreadPoolExecutor\n",
    "@log_execution\n",
    "def create_result_txt(path=\"result.txt\", directory=\"files\", num_threads=None):\n",
    "    total_files = 1000\n",
    "\n",
    "    # Open the result file in write mode\n",
    "    with open(path, 'w') as result_file:\n",
    "        # Use ThreadPoolExecutor to manage threads\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "            # Submit all file processing tasks to the thread pool\n",
    "            futures = [executor.submit(process_file, i, directory) for i in range(total_files)]\n",
    "\n",
    "            # As each task completes, write the result to the result file\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                file_name, num_lines = future.result()  # Get the result of the task\n",
    "                write_result(file_name, num_lines, result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: create_result_txt, Time taken: 0.30861830711364746 seconds\n",
      "Function: create_result_txt, Memory used: 298649 bytes, Peak Memory used: 529685 bytes\n"
     ]
    }
   ],
   "source": [
    "# Call the main function\n",
    "create_result_txt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using multi-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "# Function to read the number of lines in a file\n",
    "def read_file(file_name):\n",
    "    try:\n",
    "        with open(file_name, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            num_lines = len(lines)\n",
    "        return file_name, num_lines\n",
    "    except FileNotFoundError:\n",
    "        return file_name, 0  # Return 0 if the file does not exist\n",
    "\n",
    "# Worker function to process a batch of files and return the results\n",
    "def process_files(file_range, directory, queue):\n",
    "    results = []\n",
    "    for i in file_range:\n",
    "        file_name = f\"{directory}/file_{i+1}.txt\"\n",
    "        result = read_file(file_name)\n",
    "        results.append(result)\n",
    "    queue.put(results)  # Put the result list into the queue\n",
    "\n",
    "# Main function to create the result.txt file using multiprocessing\n",
    "@log_execution\n",
    "def create_result_txt(path=\"result.txt\", directory=\"files\", num_processes=4):\n",
    "    total_files = 1000\n",
    "    files_per_process = total_files // num_processes\n",
    "    processes = []\n",
    "    queue = multiprocessing.Queue()\n",
    "\n",
    "    # Create and start processes\n",
    "    for i in range(num_processes):\n",
    "        start = i * files_per_process + 1\n",
    "        # Ensure the last process handles any remaining files\n",
    "        end = total_files if i == num_processes - 1 else (i + 1) * files_per_process\n",
    "        file_range = range(start, end + 1)\n",
    "\n",
    "        # Create a process to handle a subset of the files\n",
    "        process = multiprocessing.Process(target=process_files, args=(file_range, directory, queue))\n",
    "        processes.append(process)\n",
    "        process.start()\n",
    "\n",
    "    # Collect results from all processes\n",
    "    all_results = []\n",
    "    for _ in range(num_processes):\n",
    "        all_results.extend(queue.get())  # Get results from the queue\n",
    "\n",
    "    # Wait for all processes to finish\n",
    "    for process in processes:\n",
    "        process.join()\n",
    "\n",
    "    # Write the results to the result.txt file\n",
    "    with open(path, 'w') as result_file:\n",
    "        for file_name, num_lines in all_results:\n",
    "            result_file.write(f\"{file_name}: {num_lines} lines\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_result_txt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time_series_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
